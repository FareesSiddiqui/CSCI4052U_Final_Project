{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import chess\n",
    "import pygame\n",
    "import torch\n",
    "from ChessPredictorClass import ChessMovePredictor\n",
    "from env_graphical import draw_board  # Import your existing draw_board function\n",
    "\n",
    "\n",
    "class ChessPygameEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gym Environment integrating Pygame for rendering and handling chess gameplay.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
    "\n",
    "    def __init__(self, model_path=\"chess_move_predictor.pth\", render_mode=\"human\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Validate render mode\n",
    "        assert render_mode in self.metadata[\"render_modes\"], f\"Invalid render_mode: {render_mode}\"\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((640, 640)) if render_mode == \"human\" else None\n",
    "        pygame.display.set_caption(\"Chess Gym Environment\")\n",
    "\n",
    "        # Initialize Chess Board\n",
    "        self.board = chess.Board()\n",
    "\n",
    "        # Load your pre-trained model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ChessMovePredictor().to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "\n",
    "        # Define observation and action spaces\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(12, 8, 8), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(64 * 64)  # Actions are from-square and to-square indices\n",
    "\n",
    "    def fen_to_tensor(self, fen):\n",
    "        \"\"\"Convert board state (FEN) to a tensor.\"\"\"\n",
    "        board = chess.Board(fen)\n",
    "        tensor = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "        piece_map = {\n",
    "            \"P\": 0, \"N\": 1, \"B\": 2, \"R\": 3, \"Q\": 4, \"K\": 5,\n",
    "            \"p\": 6, \"n\": 7, \"b\": 8, \"r\": 9, \"q\": 10, \"k\": 11\n",
    "        }\n",
    "        for square in chess.SQUARES:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece:\n",
    "                row = 7 - (square // 8)\n",
    "                col = square % 8\n",
    "                tensor[piece_map[piece.symbol()], row, col] = 1.0\n",
    "        return tensor\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment to the initial chess position.\"\"\"\n",
    "        super().reset(seed=seed)  # Seed the environment\n",
    "        self.board.reset()\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render()\n",
    "        return self.fen_to_tensor(self.board.fen()), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply an action to the board.\"\"\"\n",
    "        from_square = action // 64\n",
    "        to_square = action % 64\n",
    "        move = chess.Move(from_square, to_square)\n",
    "\n",
    "        reward = 0\n",
    "        if move in self.board.legal_moves:\n",
    "            self.board.push(move)\n",
    "\n",
    "            # Reward logic based on game state\n",
    "            if self.board.is_checkmate():\n",
    "                reward = 1  # Winning is rewarded\n",
    "                done = True\n",
    "            elif self.board.is_stalemate() or self.board.is_insufficient_material():\n",
    "                reward = 0.5  # Draw has a smaller reward\n",
    "                done = True\n",
    "            else:\n",
    "                reward = 0  # Neutral for valid moves\n",
    "                done = False\n",
    "        else:\n",
    "            reward = -1  # Penalize invalid moves\n",
    "            done = True  # End the game on invalid move\n",
    "\n",
    "        truncated = False  # Define truncation logic if necessary\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render()\n",
    "\n",
    "        return self.fen_to_tensor(self.board.fen()), reward, done, truncated, {}\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the environment.\"\"\"\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the environment.\"\"\"\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.quit()\n",
    "\n",
    "    def _render(self):\n",
    "        \"\"\"Draw the board using your existing Pygame logic.\"\"\"\n",
    "        if self.screen is not None:\n",
    "            self.screen.fill((0, 0, 0))\n",
    "            draw_board(self.screen, self.board)\n",
    "            pygame.display.flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import time\n",
    "\n",
    "class RenderCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback to render the environment during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(RenderCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.env.envs[0].render()  # Render the first environment in the vectorized wrapper\n",
    "        time.sleep(0.1)  # Delay for better visualization\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faree\\AppData\\Local\\Temp\\ipykernel_3484\\2717593892.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Create a vectorized version of the ChessPygameEnv\n",
    "env = make_vec_env(lambda: ChessPygameEnv(model_path=\"chess_move_predictor.pth\", render_mode=\"human\"), n_envs=1)\n",
    "\n",
    "# Initialize PPO with your policy model\n",
    "ppo_agent = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Create the render callback\n",
    "render_callback = RenderCallback(env)\n",
    "\n",
    "# Train the agent using self-play\n",
    "ppo_agent.learn(total_timesteps=10000, callback=render_callback)\n",
    "\n",
    "# Save the fine-tuned agent\n",
    "ppo_agent.save(\"ppo_chess_agent_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing environment with random actions...\n",
      "Action: 1769, Reward: -1, Done: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faree\\AppData\\Local\\Temp\\ipykernel_13780\\2717593892.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = ChessPygameEnv(model_path=\"chess_move_predictor.pth\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "\n",
    "print(\"Testing environment with random actions...\")\n",
    "\n",
    "while not done:\n",
    "    # Sample a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
